{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# CSE252D Spring 2022: Homework 1\n",
    "Please read README.md for detailed instructions\n",
    "## Instructions\n",
    "1. Attempt all questions.\n",
    "2. Please comment your code adequately.\n",
    "3. Include all relevant information such as text answers, output images in notebook.\n",
    "4. **Academic integrity:** The homework must be completed individually.\n",
    "\n",
    "5. **Submission instructions:**  \n",
    " (a) Submit the notebook's PDF version and your code on Gradescope.  \n",
    " (b) Rename your submission files as Lastname_Firstname.pdf and Lastname_Firstname.zip.  \n",
    " (c) Correctly select pages for each answer on Gradescope to allow proper grading.\n",
    "\n",
    "6. **Due date:** Assignments are due **Thu Apr 28 11:59PM PDT**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 1: Warm Up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "We will first try SFM using the original implementation from $\\mathtt{libviso2}$[8],[9]. We will test on a dataset containing 300 images from one sequence of the KITTI dataset with ground-truth camera poses and camera calibration information. \n",
    "\n",
    "Run the SFM algorithm using the following script. You are required to report two error metrics. The error metric for rotation is defined as the mean of Frobenius norm of the difference between the ground-truth rotation matrix and predicted rotation matrix. The error metric for translation is defined as mean of the L2 distance. Both errors will be printed on the screen as you run the code.  **(5 points)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "# change your base path\n",
    "os.chdir('./pyviso/') # './'\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "np.set_printoptions(precision=4)\n",
    "np.set_printoptions(suppress=True)\n",
    "import viso2\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.io import imread\n",
    "import time\n",
    "\n",
    "def errorMetric(RPred, RGt, TPred, TGt):\n",
    "    diffRot = (RPred - RGt)\n",
    "    diffTrans = (TPred - TGt)\n",
    "    errorRot = np.sqrt(np.sum(np.multiply(diffRot.reshape(-1), diffRot.reshape(-1))))\n",
    "    errorTrans = np.sqrt(np.sum(np.multiply(diffTrans.reshape(-1), diffTrans.reshape(-1))))\n",
    "\n",
    "    return errorRot, errorTrans\n",
    "\n",
    "if_vis = True # set to True to do the visualization per frame; the images will be saved at '.vis/'. Turn off if you just want the camera poses and errors\n",
    "if_on_screen = False # if True the visualization per frame is going to be displayed realtime on screen; if False there will be no display, but in both options the images will be saved\n",
    "\n",
    "# parameter settings (for an example, please download\n",
    "# dataset_path = '../dataset'\n",
    "dataset_path = '/datasets/cs252d-sp22-a00-public/dataset_SfM' # On ``dsmlp-login.ucsd.edu`` server\n",
    "img_dir      = os.path.join(dataset_path, 'sequences/00/image_0')\n",
    "gt_dir       = os.path.join(dataset_path, 'poses/00.txt')\n",
    "calibFile    = os.path.join(dataset_path, 'sequences/00/calib.txt')\n",
    "border       = 50;\n",
    "gap          = 15;\n",
    "\n",
    "# Load the camera calibration information\n",
    "with open(calibFile) as fid:\n",
    "    calibLines = fid.readlines()\n",
    "    calibLines = [calibLine.strip() for calibLine in calibLines]\n",
    "\n",
    "calibInfo = [float(calibStr) for calibStr in calibLines[0].split(' ')[1:]]\n",
    "# param = {'f': calibInfo[0], 'cu': calibInfo[2], 'cv': calibInfo[6]}\n",
    "\n",
    "# Load the ground-truth depth and rotation\n",
    "with open(gt_dir) as fid:\n",
    "    gtTr = [[float(TrStr) for TrStr in line.strip().split(' ')] for line in fid.readlines()]\n",
    "gtTr = np.asarray(gtTr).reshape(-1, 3, 4)\n",
    "\n",
    "# param['height'] = 1.6\n",
    "# param['pitch']  = -0.08\n",
    "# param['match'] = {'pre_step_size': 64}\n",
    "first_frame  = 0\n",
    "last_frame   = 300\n",
    "epi = 1e-8\n",
    "\n",
    "# init visual odometry\n",
    "params = viso2.Mono_parameters()\n",
    "params.calib.f = calibInfo[0]\n",
    "params.calib.cu = calibInfo[2]\n",
    "params.calib.cv = calibInfo[6]\n",
    "params.height = 1.6\n",
    "params.pitch = -0.08\n",
    "\n",
    "\n",
    "first_frame  = 0\n",
    "last_frame   = 300\n",
    "\n",
    "# init transformation matrix array\n",
    "Tr_total = []\n",
    "Tr_total_np = []\n",
    "Tr_total.append(viso2.Matrix_eye(4))\n",
    "Tr_total_np.append(np.eye(4))\n",
    "\n",
    "# init viso module\n",
    "visoMono = viso2.VisualOdometryMono(params)\n",
    "\n",
    "if if_vis:\n",
    "    save_path = 'vis'\n",
    "    os.makedirs(save_path, exist_ok=True)\n",
    "\n",
    "    # create figure\n",
    "    fig = plt.figure(figsize=(10, 15))\n",
    "    ax1 = plt.subplot(211)\n",
    "    ax1.axis('off')\n",
    "    ax2 = plt.subplot(212)\n",
    "    ax2.set_xticks(np.arange(-100, 100, step=10))\n",
    "    ax2.set_yticks(np.arange(-500, 500, step=10))\n",
    "    ax2.axis('equal')\n",
    "    ax2.grid()\n",
    "    if if_on_screen:\n",
    "        plt.ion()\n",
    "    else:\n",
    "        plt.ioff()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "# for all frames do\n",
    "if_replace = False\n",
    "errorTransSum = 0\n",
    "errorRotSum = 0\n",
    "errorRot_list = []\n",
    "errorTrans_list =[]\n",
    "\n",
    "for frame in range(first_frame, last_frame):\n",
    "    # 1-based index\n",
    "    k = frame-first_frame+1\n",
    "\n",
    "    # read current images\n",
    "    I = imread(os.path.join(img_dir, '%06d.png'%frame))\n",
    "    assert(len(I.shape) == 2) # should be grayscale\n",
    "\n",
    "    # compute egomotion\n",
    "    process_result = visoMono.process_frame(I, if_replace)\n",
    "    Tr = visoMono.getMotion()\n",
    "    matrixer = viso2.Matrix(Tr)\n",
    "    Tr_np = np.zeros((4, 4))\n",
    "    Tr.toNumpy(Tr_np) # so awkward...\n",
    "\n",
    "    # accumulate egomotion, starting with second frame\n",
    "    if k > 1:\n",
    "        if process_result is False:\n",
    "            if_replace = True\n",
    "            Tr_total.append(Tr_total[-1])\n",
    "            Tr_total_np.append(Tr_total_np[-1])\n",
    "        else:\n",
    "            if_replace = False\n",
    "            Tr_total.append(Tr_total[-1] * viso2.Matrix_inv(Tr))\n",
    "            Tr_total_np.append(Tr_total_np[-1] @ np.linalg.inv(Tr_np)) # should be the same\n",
    "            print(Tr_total_np[-1])\n",
    "\n",
    "    # output statistics\n",
    "    num_matches = visoMono.getNumberOfMatches()\n",
    "    num_inliers = visoMono.getNumberOfInliers()\n",
    "    matches = visoMono.getMatches()\n",
    "    matches_np = np.empty([4, matches.size()])\n",
    "\n",
    "    for i,m in enumerate(matches):\n",
    "        matches_np[:, i] = (m.u1p, m.v1p, m.u1c, m.v1c)\n",
    "\n",
    "    if if_vis:\n",
    "        # update image\n",
    "        ax1.clear()\n",
    "        ax1.imshow(I, cmap='gray', vmin=0, vmax=255)\n",
    "        if num_matches != 0:\n",
    "            for n in range(num_matches):\n",
    "                ax1.plot([matches_np[0, n], matches_np[2, n]], [matches_np[1, n], matches_np[3, n]])\n",
    "        ax1.set_title('Frame %d'%frame)\n",
    "\n",
    "        # update trajectory\n",
    "        if k > 1:\n",
    "            ax2.plot([Tr_total_np[k-2][0, 3], Tr_total_np[k-1][0, 3]], \\\n",
    "                [Tr_total_np[k-2][2, 3], Tr_total_np[k-1][2, 3]], 'b.-', linewidth=1)\n",
    "            ax2.plot([gtTr[k-2][0, 3], gtTr[k-1][0, 3]], \\\n",
    "                [gtTr[k-2][2, 3], gtTr[k-1][2, 3]], 'r.-', linewidth=1)\n",
    "        ax2.set_title('Blue: estimated trajectory; Red: ground truth trejectory')\n",
    "\n",
    "        plt.draw()\n",
    "\n",
    "    # Compute rotation\n",
    "    Rpred_p = Tr_total_np[k-2][0:3, 0:3]\n",
    "    Rpred_c = Tr_total_np[k-1][0:3, 0:3]\n",
    "    Rpred = Rpred_c.transpose() @ Rpred_p\n",
    "    Rgt_p = np.squeeze(gtTr[k-2, 0:3, 0:3])\n",
    "    Rgt_c = np.squeeze(gtTr[k-1, 0:3, 0:3])\n",
    "    Rgt = Rgt_c.transpose() @ Rgt_p\n",
    "    # Compute translation\n",
    "    Tpred_p = Tr_total_np[k-2][0:3, 3:4]\n",
    "    Tpred_c = Tr_total_np[k-1][0:3, 3:4]\n",
    "    Tpred = Tpred_c - Tpred_p\n",
    "    Tgt_p = gtTr[k-2, 0:3, 3:4]\n",
    "    Tgt_c = gtTr[k-1, 0:3, 3:4]\n",
    "    Tgt = Tgt_c - Tgt_p\n",
    "    # Compute errors\n",
    "    errorRot, errorTrans = errorMetric(Rpred, Rgt, Tpred, Tgt)\n",
    "    errorRotSum = errorRotSum + errorRot\n",
    "    errorTransSum = errorTransSum + errorTrans\n",
    "    # errorRot_list.append(errorRot)\n",
    "    # errorTrans_list.append(errorTrans)\n",
    "    print('Mean Error Rotation: %.5f'%(errorRotSum / (k-1+epi)))\n",
    "    print('Mean Error Translation: %.5f'%(errorTransSum / (k-1+epi)))\n",
    "    print('== [Result] Frame: %d, Matches %d, Inliers: %.2f'%(frame, num_matches, 100*num_inliers/(num_matches+1e-8)))\n",
    "\n",
    "    if if_vis:\n",
    "        # input('Paused; Press Enter to continue') # Option 1: Manually pause and resume\n",
    "        if if_on_screen:\n",
    "            plt.pause(0.1) # Or Option 2: enable to this to auto pause for a while after daring to enable animation in case of a delay in drawing\n",
    "        vis_path = os.path.join(save_path, 'frame%03d.jpg'%frame)\n",
    "        fig.savefig(vis_path)\n",
    "        print('Saved at %s'%vis_path)\n",
    "        \n",
    "        if frame % 50 == 0 or frame == last_frame-1:\n",
    "            plt.figure(figsize=(10, 15))\n",
    "            plt.imshow(plt.imread(vis_path))\n",
    "            plt.axis('off')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# input('Press Enter to exit')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Then answer the questions below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. In $\\mathtt{libviso2}$, the feature points are \"bucketed\" ($\\mathtt{src/matcher.cpp: Line 285 - 326}$), which means in a certain area of region, the number of detected keypoint pairs should be within certain bounds. Why?  **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. We have run SFM on a single camera, which means the scale of translation is unknown. However, as you may have observed, the predicted trajectory is still somewhat similar to the ground-truth trajectory. How does $\\mathtt{libviso2}$ handle this ambiguity ($\\mathtt{viso\\_mono.cpp: Line 245}$)?  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "4. Briefly explain the RANSAC algorithm used in $\\mathtt{libviso2}$ ($\\mathtt{viso\\_mono.cpp: Line 113 - 129}$).  **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 2: Using SIFT [4] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "In the second task, you are required to use keypoints and feature descriptors from SIFT for SFM. The SIFT implementation can be found in directory $\\mathtt{SIFT}$. \n",
    "\n",
    "(A) Go to $\\mathtt{SIFT}$ directory and run $\\mathtt{runSIFT.py}$ (e.g. `python runSIFT.py --input /datasets/cs252d-sp22-a00-public/dataset_SfM/sequences/00/image_0/`). You will save the detected keypoints and feature descriptors under the directory $\\mathtt{SIFT}$. For image $\\mathtt{000abc.png}$, the pre-computed features and keypoints should be saved in a $\\mathtt{.npy}$ file named as $\\mathtt{000abc\\_feature.npy}$. The variable should be a $130 \\times N$ matrix with $\\mathtt{single}$ precision, where $N$ is the number of feature points being detected. For each $130$-dimensional feature vector, the first two dimensions are the location of the keypoints (column number first and then row number) on the image plane and the last $128$ dimensions are the feature descriptor. \n",
    "\n",
    "(B) Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import runFeature\n",
    "dataset_path = '/datasets/cs252d-sp22-a00-public/dataset_SfM'\n",
    "feature_dir = 'SIFT'\n",
    "runFeature.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Does SIFT yield higher accuracy than the original $\\mathtt{libviso2}$? Why or why not? If not, can you suggest one possible way to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Explain how SIFT achieves invariance to \n",
    "       a. illumination\n",
    "       b. rotation\n",
    "       c. scale\n",
    " **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 3: Using SuperPoint[1] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now you are required to use keypoints and feature descriptors from SuperPoint for SFM. The code for the trained model of this method can be found from the $\\mathtt{SuperPoint}$.\n",
    "\n",
    "(A) Go to $\\mathtt{SuperPoint}$ directory and run $\\mathtt{demo\\_superpoint.py}$ (e.g. `python demo_superpoint.py --input /datasets/cs252d-sp22-a00-public/dataset_SfM/sequences/00/image_0/`). The detected keypoints and feature descriptors are under the directory $\\mathtt{SuperPoint}$. The file format is similar to the SIFT case. For image $\\mathtt{000abc.png}$, the pre-computed features and keypoints should be saved in a $\\mathtt{.npy}$ file named as $\\mathtt{000abc\\_feature.npy}$. The variable is a $258\\times N$ matrix with $\\mathtt{single}$ precision, where $N$ is the number of feature points being detected. For each $258$-dimensional feature vector, the first two dimensions are the locations of the keypoint (column number first and then row number) on the image plane and the last $256$ dimensions represent the feature descriptor. \n",
    "\n",
    "(B) Run the following script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import runFeature\n",
    "dataset_path = '/datasets/cs252d-sp22-a00-public/dataset_SfM'\n",
    "feature_dir = 'SuperPoint'\n",
    "runFeature.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Does SuperPoint yield higher accuracy than the original $\\mathtt{libviso2}$? If so, why? If not, what steps can you take to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Explain briefly how the following issues are being handled in SuperPoint: **(3 points)**\n",
    "       a. Obtaining ground truth for keypoints.\n",
    "       b. Cheaply obtaining accurate ground truth matches, as compared to LIDAR in UCN or SFM in LIFT.\n",
    "       c. Learning a correlated feature representation for keypoint detection and description? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 4: Using SPyNet [5] for SFM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Now we will compute camera motion from optical flow computed using SPyNet. We first uniformly sample points in an image, then consider the flow-displaced point in the other image as a match. A modified PyTorch implementation of SPyNet is provided in directory  $\\mathtt{Flow}$.\n",
    "\n",
    "(A) Go to $\\mathtt{Flow}$ and run $\\mathtt{demo\\_spynet.py}$. (e.g. `python demo_spynet.py --input /datasets/cs252d-sp22-a00-public/dataset_SfM/sequences/00/image_0/`). \n",
    "\n",
    "(B) Run the following script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "outputs": [],
   "source": [
    "import runMatch\n",
    "dataset_path = '/datasets/cs252d-sp22-a00-public/dataset_SfM'\n",
    "feature_dir = 'Flow'\n",
    "runMatch.runSFM(dataset_path, feature_dir )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Report the final rotation and translation error. **(2 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``report two error metrics over all frames here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "Next, answer the following questions:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Does SPyNet yield higher accuracy than the original $\\mathtt{libviso2}$? Why or why not? If not, what steps can you take to improve? **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. Explain how SPyNet achieves accurate flow with significantly lower computational cost. **(3 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Question 5: Using Sfm-learner for SFM\n",
    "Now we will use deep neural networks for SFM. Please follow the open-source Sfmlearner repository (https://github.com/ClementPinard/SfmLearner-Pytorch) and the paper (https://people.eecs.berkeley.edu/~tinghuiz/projects/SfMLearner/cvpr17_sfm_final.pdf) to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "1. Describe how photometric loss is implemented in Sfmlearner? Please refer to the paper and the code. **(5 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "2. Train the model and show the training curve, validation curve, and visualization of predicted depth and warped images? Which loss is decreasing and which is increasing? Why? **(10 points)**\n",
    "\n",
    " Visualize the specific images: `val_Depth_Output_Normalized/0`, `val_Warped_Outputs/0` from tensorboard.\n",
    " \n",
    " Training data: \n",
    "`/datasets/cs252d-sp22-a00-public/sfmlearner_h128w416`.\n",
    "\n",
    "#### Install the environment\n",
    "`pip install -r requirements.txt --user`\n",
    "- fix scipy version problem: \n",
    "\n",
    "`pip install scipy==1.1.0 --user`\n",
    "\n",
    "#### Training script\n",
    "`cd SfmLearner-Pytorch`\n",
    "\n",
    "`python3 train.py /datasets/cs252d-sp22-a00-public/sfmlearner_h128w416 -b4 -m0.2 -s0.1 --epoch-size 3000 --sequence-length 3 --log-output`\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "3. When training the model, we use 3 consecutive frames. Now, you will use the photometric consistency between the 1st and the 3rd frame. To be more specific, you can get the pose $T_{1,3} = T_{2,3} @ T_{1,2}$, where $T_{1,2}, T_{2,3}$ have already been computed in the original code. Add the constraint to the total loss and report the results in the same manner as in part 2 above. You can simply ignore the explainability mask between 1st and 3rd frame. **(10 points)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "4. Now, you will evaluate your models from parts (2), (3) on the KITTI odometry dataset. What are the error metrics used in Sfm-learner? Please report the `ATE` and `RE` for sequence `09` and `10`. **(5 points)**\n",
    "\n",
    "  Data: `/datasets/cs252d-sp22-a00-public/kitti`.\n",
    "\n",
    "#### Evaluation script\n",
    "`python3 test_pose.py /path/to/posenet --dataset-dir /datasets/cs252d-sp22-a00-public/kitti --sequences 09`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "``answer here``"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "### Notes:\n",
    "- scp data to local machines:\n",
    "\n",
    "`scp <USERNAME>@dsmlp-login.ucsd.edu:/datasets/cs252d-sp22-a00-public/sfmlearner_h128w416.zip <LOCAL PATH>`\n",
    "\n",
    "`scp <USERNAME>@dsmlp-login.ucsd.edu:/datasets/cs252d-sp22-a00-public/kitti.zip <LOCAL PATH>`\n",
    "...\n",
    "- tensorboard: open jupyter notebook from the link after launching the container. Go to checkpoint directory and click new -> `Tensorboard`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "# References\n",
    "1. Daniel DeTone, Tomasz Malisiewicz, and Andrew Rabinovich. Superpoint: Self-supervised interest point detection and description. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition Workshops, pages 224–236, 2018.\n",
    "2. Andreas Geiger, Philip Lenz, and Raquel Urtasun. Are we ready for autonomous driving? the kitti vision benchmark suite. In Conference on Computer Vision and Pattern Recognition (CVPR), 2012.\n",
    "3. Andreas Geiger, Julius Ziegler, and Christoph Stiller. Stereoscan: Dense 3d reconstruction in real-time. In Intelligent Vehicles Symposium (IV), 2011.\n",
    "4. David G Lowe. Distinctive image features from scale-invariant keypoints. IJCV, 60(2):91–110, 2004.\n",
    "5. Anurag Ranjan and Michael J Black. Optical flow estimation using a spatial pyramid network. In\n",
    "Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages 4161–4170, 2017.\n",
    "6. A. Vedaldi and B. Fulkerson. VLFeat: An open and portable library of computer vision algorithms. http://www.vlfeat.org/, 2008.\n",
    "7. Lucas, Bruce D., and Takeo Kanade. \"An iterative image registration technique with an application to stereo vision.\" (1981): 674.\n",
    "8. B. Kitt, A. Geiger, and H. Lategahn, “Visual odometry based on stereo image sequences with RANSAC-based outlier rejection scheme,” in 2010 IEEE Intelligent Vehicles Symposium, La Jolla, CA, USA, Jun. 2010, pp. 486–492, doi: 10.1109/IVS.2010.5548123.\n",
    "9. A. Geiger, J. Ziegler, and C. Stiller, “StereoScan: Dense 3d reconstruction in real-time,” in 2011 IEEE Intelligent Vehicles Symposium (IV), Baden-Baden, Germany, Jun. 2011, pp. 963–968, doi: 10.1109/IVS.2011.5940405."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.10"
     }
    }
   ],
   "remote_diff": [
    {
     "key": "kernelspec",
     "op": "add",
     "value": {
      "display_name": "py36-torch",
      "language": "python",
      "name": "myenv"
     }
    },
    {
     "key": "language_info",
     "op": "add",
     "value": {
      "codemirror_mode": {
       "name": "ipython",
       "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
     }
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
